{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0UAJY7UjwsJ"
      },
      "outputs": [],
      "source": [
        "# \n",
        "!pip install -q google.generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9cOwbRkjwsV"
      },
      "outputs": [],
      "source": [
        "# This script demonstrates how to use the PaLM API with Google Cloud credentials.\n",
        "# It assumes you have a file named credentials.py with the following content:\n",
        "import os\n",
        "import google.generativeai as palm\n",
        "from google.api_core import client_options\n",
        "# Get credentials from credentials.py\n",
        "from credentials import credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1rCTzvwjwsX"
      },
      "outputs": [],
      "source": [
        "# Configure PALM client\n",
        "palm.configure(\n",
        "    api_key = credentials[\"api_key\"], # API key\n",
        "    transport = \"rest\", # Use REST transport for local development\n",
        "    client_options = client_options.ClientOptions(api_endpoint=os.getenv(\"GOOGLE_API_BASE\")), # Use local API endpoint for local development\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCe6CAQZjwsZ",
        "outputId": "3f567b29-81f0-4027-86e5-80473f0dd6ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model {'models/chat-bison-001'}\n",
            "description: Chat-optimized generative language model.\n",
            "Supported Generation Methods: ['generateMessage', 'countMessageTokens']\n",
            "Model {'models/text-bison-001'}\n",
            "description: Model targeted for text generation.\n",
            "Supported Generation Methods: ['generateText', 'countTextTokens', 'createTunedTextModel']\n",
            "Model {'models/embedding-gecko-001'}\n",
            "description: Obtain a distributed representation of a text.\n",
            "Supported Generation Methods: ['embedText', 'countTextTokens']\n"
          ]
        }
      ],
      "source": [
        "# Explore available models & their generation methods\n",
        "for model in palm.list_models():\n",
        "    print(\"Model\",{model.name})\n",
        "    print(f\"description: {model.description}\")\n",
        "    print(\"Supported Generation Methods:\", model.supported_generation_methods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6m4XHbMjwsb"
      },
      "outputs": [],
      "source": [
        "# Save models that support text generation\n",
        "text_generators = [model for model in palm.list_models() if \"generateText\" in model.supported_generation_methods]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZXP08YHjwsd"
      },
      "outputs": [],
      "source": [
        "text_gen_model = text_generators[0] # Use the first model that supports text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBvOyDOWmCR0",
        "outputId": "cd15a623-769d-480d-e71d-bb73fe3363d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Model(name='models/text-bison-001',\n",
              "      base_model_id='',\n",
              "      version='001',\n",
              "      display_name='Text Bison',\n",
              "      description='Model targeted for text generation.',\n",
              "      input_token_limit=8196,\n",
              "      output_token_limit=1024,\n",
              "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "      temperature=0.7,\n",
              "      top_p=0.95,\n",
              "      top_k=40)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_gen_model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shP1Yvyzmkf-",
        "outputId": "2271026f-5a5e-4ead-e927-8a87cee300d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! How can I help you today?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ChatResponse(model='models/chat-bison-001',\n",
              "             context='',\n",
              "             examples=[],\n",
              "             messages=[...],\n",
              "             temperature=None,\n",
              "             candidate_count=None,\n",
              "             candidates=[...],\n",
              "             filters=[],\n",
              "             top_p=None,\n",
              "             top_k=None,\n",
              "             _client=<google.ai.generativelanguage_v1beta3.services.discuss_service.client.DiscussServiceClient object at 0x7f68b9688910>)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = palm.chat(messages=[\"Hello.\"]) # Generate a response to the message \"Hello.\"\n",
        "print(response.last) # Print the last message in the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euhDwiUpmzXE",
        "outputId": "e03d0474-aac1-467f-c385-0b07829a9efd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatResponse(model='models/chat-bison-001',\n",
              "             context='',\n",
              "             examples=[],\n",
              "             messages=[...],\n",
              "             temperature=None,\n",
              "             candidate_count=None,\n",
              "             candidates=[...],\n",
              "             filters=[],\n",
              "             top_p=None,\n",
              "             top_k=None,\n",
              "             _client=<google.ai.generativelanguage_v1beta3.services.discuss_service.client.DiscussServiceClient object at 0x7f68b9688910>)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.reply(\"I need help with django\") # Generate a reply to the message \"I need help with django.\"\n",
        "print(response.last) # Print the last message in the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.api_core import retry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkYr5Qy-jwsf"
      },
      "outputs": [],
      "source": [
        "@retry.Retry(deadline=60, predicate=retry.if_exception_type(Exception)) # Retry the function if it raises an exception\n",
        "def generate_text(prompt, model = text_gen_model,temperature = 0.1): # Generate text using the specified model and prompt\n",
        "    return palm.generate_text(prompt=prompt, model=model, temperature=temperature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTfucSuWjwsh"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"I have a django project where I make users upload images and then I process them using this view but I want an other view that makes the processing for a displayed image selected by a user: def upload_image(request):\n",
        "    \\\"\\\"\\\"\n",
        "    Handles image upload, processing, and classification.\n",
        "\n",
        "    Inputs:\n",
        "    - request (HttpRequest): The HTTP request object containing the uploaded image.\n",
        "\n",
        "    Outputs:\n",
        "    - JsonResponse: A JSON response containing prediction results or an error message.\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    if request.method == 'POST':\n",
        "        form = UploadImageForm(request.POST, request.FILES)\n",
        "        if form.is_valid():\n",
        "\n",
        "            image_instance = form.save(commit=False)\n",
        "\n",
        "            image = Image.open(image_instance.image)\n",
        "\n",
        "            # Convert the image to grayscale\n",
        "            grayscale = convert_to_grayscale(image)\n",
        "\n",
        "            # Preprocess the image for the model\n",
        "            preprocessed_image = preprocess_image(grayscale, target_size=input_shape)\n",
        "\n",
        "            # Expand dimensions to match model input\n",
        "            preprocessed_image = np.expand_dims(preprocessed_image, axis=0)\n",
        "            print('predicting...')\n",
        "\n",
        "            # Perform the image classification and get a prediction\n",
        "            prediction = loaded_model.predict(preprocessed_image)\n",
        "            print('prediction done')\n",
        "            # Determine the predicted label based on the prediction score\n",
        "            if prediction <= 0.3:\n",
        "                predicted_label = 'Defected Condition'\n",
        "            elif prediction > 0.3:\n",
        "                predicted_label = 'Non Defected Condition'\n",
        "            else:\n",
        "                predicted_label = 'unknown, there must be an error'\n",
        "            # Convert the original image to bytes\n",
        "            original_image_data = image.tobytes()\n",
        "            # Create a unique blob name based on the prediction label\n",
        "            timestamp = timezone.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "            blob_name = f\"{timestamp}_{predicted_label}\"\n",
        "\n",
        "            # Upload the image to Azure Blob Storage\n",
        "            upload_result = upload_image_to_blob(original_image_data, blob_name)\n",
        "            print('upload done')\n",
        "            if upload_result:\n",
        "\n",
        "                print('returning response')\n",
        "                # Image was successfully uploaded to Azure Blob Storage\n",
        "                # Save the image instance with the blob_name\n",
        "                image_instance.blob_name = blob_name\n",
        "                image_instance.save()\n",
        "                response_data = {\n",
        "                    'prediction': predicted_label,\n",
        "                    'prediction_probabilityOK': prediction.tolist()[0][0],\n",
        "                    'prediction_probabilityDefect': 1- prediction.tolist()[0][0],\n",
        "                }\n",
        "                return JsonResponse(response_data)\n",
        "            else:\n",
        "                # Handle the case where the image upload to Blob Storage failed\n",
        "                response_data = {\n",
        "                    'error': 'Failed to upload image to Azure Blob Storage',\n",
        "                }\n",
        "                return JsonResponse(response_data, status=500)  # Return a 500 Internal Server Error status\n",
        "\n",
        "    else:\n",
        "        form = UploadImageForm()\n",
        "    return render(request, 'ClassifierModel/upload_image.html', {'form': form})\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lBi-rdHjwsi"
      },
      "outputs": [],
      "source": [
        "completion = generate_text(prompt) # Generate text using the PaLM API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et-ZXrCKjwsk",
        "outputId": "564bbd05-4f26-4d8b-ad67-fb0b45953168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "def process_image(request, image_id):\n",
            "    \"\"\"\n",
            "    Processes an image that has been selected by a user.\n",
            "\n",
            "    Inputs:\n",
            "    - request (HttpRequest): The HTTP request object.\n",
            "    - image_id (str): The ID of the image to be processed.\n",
            "\n",
            "    Outputs:\n",
            "    - JsonResponse: A JSON response containing prediction results or an error message.\n",
            "    \"\"\"\n",
            "\n",
            "    # Get the image instance from the database\n",
            "    image_instance = Image.objects.get(id=image_id)\n",
            "\n",
            "    # Get the image data from Azure Blob Storage\n",
            "    image_data = get_image_from_blob(image_instance.blob_name)\n",
            "\n",
            "    # Convert the image to grayscale\n",
            "    grayscale = convert_to_grayscale(image_data)\n",
            "\n",
            "    # Preprocess the image for the model\n",
            "    preprocessed_image = preprocess_image(grayscale, target_size=input_shape)\n",
            "\n",
            "    # Expand dimensions to match model input\n",
            "    preprocessed_image = np.expand_dims(preprocessed_image, axis=0)\n",
            "\n",
            "    # Perform the image classification and get a prediction\n",
            "    prediction = loaded_model.predict(preprocessed_image)\n",
            "\n",
            "    # Determine the predicted label based on the prediction score\n",
            "    if prediction <= 0.3:\n",
            "        predicted_label = 'Defected Condition' \n",
            "    elif prediction > 0.3:\n",
            "        predicted_label = 'Non Defected Condition'\n",
            "    else:\n",
            "        predicted_label = 'unknown, there must be an error'\n",
            "\n",
            "    # Create a response object\n",
            "    response_data = {\n",
            "        'prediction': predicted_label,\n",
            "        'prediction_probabilityOK': prediction.tolist()[0][0],\n",
            "        'prediction_probabilityDefect': 1- prediction.tolist()[0][0],    \n",
            "    }\n",
            "\n",
            "    # Return the response object\n",
            "    return JsonResponse(response_data)\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(completion.result) # Print the generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDB0009xjwsl"
      },
      "outputs": [],
      "source": [
        "prompt = \"Show me how to use docker for my machine learning projects\" # Set a prompt for the PaLM API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHoHvhKvjwsl"
      },
      "outputs": [],
      "source": [
        "completion = generate_text(prompt) # Generate text using the PaLM API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKexyadljwsm"
      },
      "outputs": [],
      "source": [
        "# Set a prompt for the PaLM API\n",
        "prompt = \"Based on the Idea that: 'In machine learning,there is a model centric approach and a data centric approach' generate 10 interview questions.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3eMx2xpjwsm"
      },
      "outputs": [],
      "source": [
        "# Generate text using the PaLM API\n",
        "completion = generate_text(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPwpVUoajwsn",
        "outputId": "8a0bcc79-b10e-44ea-9643-2c9fdbca2b4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. What is the difference between a model-centric approach and a data-centric approach to machine learning?\n",
            "2. What are the advantages and disadvantages of each approach?\n",
            "3. When is it better to use a model-centric approach?\n",
            "4. When is it better to use a data-centric approach?\n",
            "5. How can you decide which approach to use for a particular problem?\n",
            "6. What are some of the challenges of using a model-centric approach?\n",
            "7. What are some of the challenges of using a data-centric approach?\n",
            "8. How can you mitigate the challenges of each approach?\n",
            "9. What are the latest research advances in model-centric and data-centric machine learning?\n",
            "10. What are the future trends in model-centric and data-centric machine learning?\n"
          ]
        }
      ],
      "source": [
        "print(completion.result) # View the generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the generated text to a file\n",
        "with open(\"interview_questions.txt\", \"w\") as f:\n",
        "    f.write(completion.result) # Write the generated text to the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br6-yWCdjwsn"
      },
      "outputs": [],
      "source": [
        "# Save models that support text generation\n",
        "chat_generators =    [model for model in palm.list_models() if \"generateMessage\" in model.supported_generation_methods] # "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNaNAHr8jwso"
      },
      "outputs": [],
      "source": [
        "chat_gen_model =chat_generators[0] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHpliJN4jwso"
      },
      "outputs": [],
      "source": [
        "# Set prompt template\n",
        "prompt_template = \"\"\"\n",
        "{priming}\n",
        "\n",
        "{question}\n",
        "\n",
        "{decorator}\n",
        "\n",
        "solution:\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl-Yj-GUjwsp"
      },
      "outputs": [],
      "source": [
        "# Set prompt variables\n",
        "priming_text = \"You are an expert at writing clear, concise, Python code.\"\n",
        "question = \"create a doubly linked list\"\n",
        "decorator = \"The code should be well documented and include type hints.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format the prompt using the template and variables\n",
        "prompt = prompt_template.format(priming=priming_text,\n",
        "                                question=question,\n",
        "                                decorator=decorator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFg_d7n_jwsq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
